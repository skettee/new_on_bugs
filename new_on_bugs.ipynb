{"cells":[{"cell_type":"markdown","source":[" ## 벅스 최신음악 🤖\n","\n"," 벅스의 최신 음악을 모아주는 봇입니다.\n","\n"," - 개발자: skettee\n","\n"," - 깃허브 주소: [new_on_bugs](https://github.com/skettee/new_on_bugs)\n","\n","\n"," ### 개발 환경 만들기\n","\n"," 봇을 개발하기 위해서는 몇가지 소프트웨어를 설치하고 환경을 설정해야 합니다.\n"," [개발 환경 만들기](https://github.com/moabogey/docs/wiki/개발환경만들기)를 참조 하세요.\n","\n"," ### 코드 실행\n","\n"," - 터미널 실행\n","\n","   - 🖼  Windows PowerShell을 실행한다.\n","\n","   - 🍎 Terminal을 실행한다.\n","\n"," - 작업할 폴더를 생성한다.\n","\n"," ```\n"," mkdir MyWork\n"," ```\n","\n"," - 작업할 폴더로 이동한다.\n","\n"," ```\n"," cd MyWork\n"," ```\n","\n"," - 깃 클론 (Git Clone)을 수행한다.\n","\n"," ```\n"," git clone https://github.com/skettee/new_on_bugs.git\n"," ```\n","\n"," - 복사한 코드의 폴더로 이동한다.\n","\n"," ```\n"," cd new_on_bugs\n"," ```\n","\n"," - VSCode를 실행한다.\n","\n"," ```\n"," code .\n"," ```\n","\n"," - 왼쪽 EXPLORE에서 `new_on_bugs.py`를 선택한다.\n","\n"," - 하단 바에 `Python3.7.3 64-bit('base':conda)`를 누른다.\n","\n"," - `Python 3.6.8 64-bit ('moabogey':conda)`를 선택한다.\n","\n"," - 소스 코드에 `RunCell | Run Below`에서 `Run Below`를 누른다.\n","\n"," - 데이터가 정상적으로 수집이 되는지 오른쪽 Python Interactive에서 확인한다.\n","\n","\n"," ### 코드 분석\n","\n"," new_on_bugs.py를 분석합니다.\n"," 봇의 소스 코드는 크게 세단계로 나눌 수 있습니다.\n","\n"," 1. 사이트의 HTML에서 데이터를 수집\n","\n"," 2. 포스트의 HTML에서 데이터를 수집\n","\n"," 3. 데이터 저장\n","\n","\n"," **사이트의 HTML에서 데이터를 수집**\n","\n"," - 데이터를 수집할 사이트의 정보와 주소를 설정합니다. 여기에서는 https://music.bugs.co.kr/newest 에서 데이터를 수집합니다.\n","\n"," - requests와 beautifulsoup4를 이용해서 사이트의 HTML을 가져오고 파일로 저장합니다.\n","\n"," - 저장된 HTML파일 (bugs_source.html)을 열어 봅니다. 여기서 우리는 \"포스트의 리스트\"를 표현하는 구간을 찾을 것입니다. **포스트**는 제목, 내용, 이미지, 작성자, 작성 날짜 및 페이지 위치(URL)를 가지고 있는 하나의 문서를 나타내는 용어로 사용합니다.\n","\n"," ```\n"," +-------------- +-> <ul class=\"... albumList\">\n"," +\n"," +-------------+ +->   <figure albumid=...>\n"," |   Post 1\n"," |  (Item 1)\n"," +-------------+ +->   </figure>\n","\n"," +-------------+ +->   <figure albumid=...>\n"," |   Post 2\n"," |  (Item 2)\n"," +-------------+ +->   </figure>\n","\n"," +-------------+ +->  <figure albumid=...>\n"," |   Post3\n"," |  (Item 3)\n"," +-------------+ +->   </figure>\n"," ```\n","\n"," - 각각의 포스트는 `<figure albumid=...>` 에서 시작 되고 `</figure>`로 끝난다는 것을 알아내는 것이 중요합니다. 이것은 사이트마다 다르기 때문에 이것을 찾아내는 것은 약간의 경험이 필요합니다.\n","\n"," - 발견한 포스트에서 아래와 같이 제목, 작성자, 올린 시간 및 포스트 위치(URL)를 찾습니다.\n","\n"," ```\n"," +-------------+\n"," |   Post 1\n"," +-------------+\n"," |  title      +->  <a class=\"albumTitle\"... title=...>\n"," |\n"," |  post URL   +->  <a class=\"albumTitle\"... href=...>\n"," |\n"," |  createdBy  +->  <a class=\"artistTitle\"... title=...>\n"," |\n"," |  createdAt  +-> <time datetime=...>\n"," +-------------+\n"," ```\n","\n"," - 나머지 데이터를 수집하기 위해서 포스트 HTML로 이동합니다.\n","\n","\n"," **포스트의 HTML에서 데이터를 수집**\n","\n"," - 데이터를 수집할 포스트의 주소를 설정합니다.\n","\n"," - requests와 beautifulsoup4를 이용해서 사이트의 HTML을 가져오고 파일로 저장합니다.\n","\n"," - 저장된 HTML파일 (bugs_post_source.html)을 열어 봅니다. 여기서 우리는 포스트의 이미지를 수집할 것입니다.\n","\n","\n"," **Open Graph Protocol**\n","\n"," - 대부분의 사이트들은 우리가 수집할 데이터를 사이트의 첫머리에 미리 모아 놓고 있습니다. 이 규약(Protocol)은 사이트를 모두 분석하지 않고도 사이트의 내용을 파악하는데 도움이 됩니다.\n","\n"," - 아래와 같은 메타 태그를 사용합니다.\n","\n"," ```html\n"," <head>\n"," ...\n","     <meta content=\"...\" property=\"og:url\"/>\n","     <meta content=\"...\" property=\"og:title\"/>\n","     <meta content=\"...\" property=\"og:image\"/>\n","     <meta content=\"...\" property=\"og:description\"/>\n","     <meta content=\"...\" property=\"og:site_name\"/>\n"," ...\n"," </head>\n"," ```\n","\n"," - 메타 태그에서 데이터를 수집합니다.\n","\n"," - 메타 태그의 정보가 부족한 경우에는 본문에서 직접 데이터를 수집합니다.\n","\n","\n"," **데이터 저장**\n","\n"," - 수집한 데이터를 선별해서 중복되는 것을 제외하고 데이터베이스에 저장합니다. 모아보기 봇은 하루에 24번 이상 동작 하도록 되어 있기 때문에 한번에 모든 데이터를 수집하지 않고 가장 최근의 데이터 1~2개를 수집하는 것이 원칙입니다.\n","\n","\n"," ### 참고 사이트\n","\n"," - [개발 환경 만들기](https://github.com/moabogey/docs/wiki/개발환경만들기)\n","\n"," - [예제 코드 실행](https://github.com/moabogey/docs/wiki/예제코드실행)\n","\n"," - [코딩을 하기 전에](https://github.com/moabogey/docs/wiki/코딩하기전에)\n","\n"," - [예제 코드 분석](https://github.com/moabogey/docs/wiki/예제코드분석)\n","\n"," - [봇 개발 하기](https://github.com/moabogey/docs/wiki/봇개발하기)\n","\n","\n"," ### ⬇️소스코드"],"metadata":{}},{"source":["import requests\n","from requests.exceptions import HTTPError\n","from bs4 import BeautifulSoup\n","import re\n","import json\n","from datetime import datetime\n","from datetime import timedelta\n","\n","if __debug__:\n","    import os.path\n","\n","# 모아보기 컴포넌트를 가져온다.\n","import moabogey_database as moabogey\n","from moabogey_id import *\n","\n","# 사이트 이름\n","site_name = 'bugs'\n","# 사이트에서 가져올 주제\n","subject_name = 'newest'\n","# 사이트 주소\n","site_url = 'https://music.bugs.co.kr/newest'\n","if __debug__:\n","    print('{} 데이터 수집 중...'.format(site_url))\n","\n","# 사이트의 HTML을 가져온다.\n","try:\n","    response = requests.get(site_url)\n","    # 에러가 발생 했을 경우 에러 내용을 출력하고 종료한다.\n","    response.raise_for_status()\n","except HTTPError as http_err:\n","    print(f'HTTP error occurred: {http_err}')\n","except Exception as err:\n","        print(f'Other error occurred: {err}')\n","else:\n","    html_source = response.text\n","    #print(response.status_code)\n","    #print(html_source)\n","    \n","    # BeautifulSoup 오브젝트를 생성한다.\n","    soup = BeautifulSoup(html_source, 'html.parser')\n","    \n","    # HTML을 분석하기 위해서 페이지의 소스를 파일로 저장한다.\n","    if __debug__:\n","        file_name = site_name + '_source.html'\n","        if not os.path.isfile(file_name):\n","            print('file save: ', file_name)\n","            with open(file_name, 'w', encoding='utf-8') as f:\n","                f.write(soup.prettify())\n","       \n","    # 데이터를 저장할 데이터베이스를 생성한다. \n","    # bot_id는 moabogey_id에서 가져온 값이다.\n","    db_name = subject_name + '_on_' + site_name \n","    my_db = moabogey.Dbase(db_name, bot_id)\n","            \n","    # 사이트 이름을 수집한다.\n","    moa_site_name = soup.find('meta', property=\"og:site_name\")\n","    if moa_site_name:\n","        moa_site_name = moa_site_name['content']\n","    \n","    # 앨범 목록을 찾는다.\n","    albumList = soup.find('ul', class_='albumList')\n","    #print(albumList)\n","\n","    # 반복해서 리스트의 목록을 하나씩 검색하고 데이터를 수집한다.\n","    for post in albumList.find_all('figure', albumid=True):\n","        #moa_image = post.find('img', src=True)\n","        #if moa_image:\n","        #    moa_image = moa_image['src']\n","        #print('image: ', moa_image)\n","\n","        moa_title = post.find('a', class_='albumTitle')['title']\n","        #print('title: ', moa_title)\n","\n","        moa_url = post.find('a', class_='albumTitle')['href']\n","        #print('url: ', moa_url)\n","\n","        moa_createdBy = post.find('a', class_='artistTitle')['title']\n","        moa_createdAt = post.find('time', datetime=True).text.strip()\n","        # String to DateTime format으로 변환한다.\n","        # \"2019.03.21\"\n","        moa_createdAt = datetime.strptime(moa_createdAt, '%Y.%m.%d') \n","        # UTC 값으로 변경하기 위해서 9시간을 뺀다. \n","        moa_createdAt = moa_createdAt - timedelta(hours=9)\n","        #print('moa_createdAt', moa_createdAt)\n","        \n","        # 현재 날짜와 시간을 수집한다.\n","        moa_timeStamp = datetime.now()\n","\n","        # 포스트의 주소로 이동한다.\n","        try:\n","            response =  requests.get(moa_url)\n","            response.raise_for_status()\n","        except HTTPError as http_err:\n","            print(f'HTTP error occurred: {http_err}')\n","        except Exception as err:\n","                print(f'Other error occurred: {err}')\n","        else:\n","            subhtml_source = response.text\n","            \n","            # BeautifulSoup 오브젝트를 생성한다.\n","            post = BeautifulSoup(subhtml_source, 'html.parser')\n","\n","            # HTML을 분석하기 위해서 포스트의 소스를 파일로 저장한다.\n","            if __debug__:\n","                file_name = site_name + '_post_source.html'\n","                if not os.path.isfile(file_name):\n","                    print('file save: ', file_name)\n","                    with open(file_name, 'w', encoding='utf-8') as f:\n","                        f.write(post.prettify())\n","            \n","            # 포스트의 대표 이미지(앨범 표지)의 주소를 수집한다.\n","            moa_image = post.find('meta', property=\"og:image\")\n","            if moa_image:\n","                moa_image = moa_image['content']\n","            #print('image: ', moa_image)\n","        \n","            # 데이터베이스에 있는 포스트와 중복되는지를 확인하고 \n","            # JSON형식으로 수집한 데이터를 변환한다.\n","            if my_db.isNewItem('title', moa_title):\n","                # 데이터 타입을 확인한다.\n","                assert type(moa_title) == str, 'title: type error'\n","                assert type(moa_url) == str, 'url: type error'\n","                assert type(moa_image) == str, 'image: type error'\n","                assert type(moa_site_name) == str, 'siteName: type error'\n","                assert type(moa_createdBy) == str, 'createBy: type error'\n","                assert type(moa_createdAt) == datetime, 'createdAt: type error'\n","                assert type(moa_timeStamp) == datetime, 'timeStamp: type error'\n","\n","                db_data = { 'title': moa_title, \n","                    'url': moa_url,\n","                    'image': moa_image,\n","                    'siteName': moa_site_name,\n","                    'createdBy': moa_createdBy,\n","                    'createdAt': moa_createdAt,\n","                    'timeStamp': moa_timeStamp\n","                }\n","\n","                if __debug__:\n","                    # 디버그를 위해서 수집한 데이터를 출력한다.\n","                    temp_data = db_data.copy()\n","                    #temp_data['desc'] = temp_data['desc'][:20] + '...'\n","                    print('collected json data: ')\n","                    print(json.dumps(temp_data, indent=4, ensure_ascii=False, default=str))\n","\n","                # 수집한 데이터를 데이터베이스에 전송한다.\n","                my_db.insertTable(db_data)\n","\n","                # 수집이 완료되면 프로그램을 종료한다.\n","                break\n","                \n","    # 데이터 베이스에 저장된 데이터를 디스플레이 한다.\n","    if __debug__:\n","        my_db.displayHTML()\n","        \n","    # 데이터 베이스를 닫는다.\n","    my_db.close()\n",""],"cell_type":"code","outputs":[{"output_type":"stream","name":"stdout","text":"https://music.bugs.co.kr/newest 데이터 수집 중...\ncollected json data: \n{\n    \"title\": \"WE\",\n    \"url\": \"https://music.bugs.co.kr/album/20253591?wl_ref=list_ab_03\",\n    \"image\": \"https://image.bugsm.co.kr/album/images/500/202535/20253591.jpg\",\n    \"siteName\": \"벅스!\",\n    \"createdBy\": \"WINNER\",\n    \"createdAt\": \"2019-05-14 15:00:00\",\n    \"timeStamp\": \"2019-05-16 00:36:33.668315\"\n}\n"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n  .card {\n    width: 340px;\n    margin-bottom: 20px;\n    border: 1px solid #DDDDDD;\n    border-radius: 4px;\n    overflow: hidden;\n    display: flex;\n    flex-direction: column;\n  }\n  .center {\n    display: block;\n    width: 100%;\n    margin-left: auto;\n    margin-right: auto;\n  }\n  .card_item {\n    width: 340px;\n    padding: 5px 10px;\n  }\n  hr { \n    display: flex;\n    margin-top: 0.5em;\n    margin-bottom: 0.5em;\n    margin-left: auto;\n    margin-right: 100px;\n    border-style: inset;\n    border-width: 1px;\n  } \n  h5 {\n  \tcolor: blue;\n  }\n</style>\n<div class=\"card\">\n  <img src=\"https://image.bugsm.co.kr/album/images/500/202535/20253591.jpg\" alt=\"Image\" class=\"center\">\n  <div class=\"card_item\">\n    <h5>벅스!</h5>\n    <h4><b>WE</b></h4> \n    <hr>\n    <p> By WINNER </p>\n  </div>\n</div>\n"},"metadata":{}}],"metadata":{},"execution_count":1}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}